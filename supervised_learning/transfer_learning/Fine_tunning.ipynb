{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9ad27c",
   "metadata": {},
   "source": [
    "# Fine Tunning\n",
    "\n",
    "In the pursuit of enhancing model performance, I ventured into fine-tuning, a sophisticated technique that involves adjusting the parameters of a pre-trained model to better suit the characteristics of a specific dataset or task.\n",
    "\n",
    "In my fine-tuning endeavor, I adopted a structured approach to model creation and hyperparameter optimization. Leveraging the Keras Tuner library, I designed a custom model-building function capable of constructing neural networks with various configurations, including adjustable dense layers with regularization techniques like L1 and L2 regularization. This function allowed for comprehensive exploration of hyperparameter space, enabling the identification of optimal model architectures tailored to the CIFAR-10 dataset.\n",
    "\n",
    "To facilitate fine-tuning, I strategically froze a subset of layers within the InceptionResNetV2 base model while leaving others trainable. This selective freezing approach helps preserve the pre-learned representations in lower layers while allowing higher-level features to adapt to the new task. By fine-tuning the model's parameters, I aimed to strike a balance between leveraging the generic features learned from ImageNet and tailoring the model to the nuances of CIFAR-10.\n",
    "\n",
    "Hyperparameter optimization played a pivotal role in fine-tuning, guiding the search for optimal model configurations. The Hyperband algorithm facilitated efficient exploration of hyperparameter space, iteratively refining model architectures based on performance metrics such as validation accuracy. Through this iterative process, I identified the best combination of hyperparameters, including the number of units in dense layers, learning rate, dropout rate, and regularization strength.\n",
    "\n",
    "Upon identifying the optimal hyperparameters, I constructed the final model and conducted training sessions to refine its parameters further. The trained model exhibited improved performance, achieving higher accuracy on both the training and validation datasets. Encouraged by these results, I preserved the best-performing model by saving it to disk, ensuring its availability for future deployment and inference tasks.\n",
    "\n",
    "In summary, fine-tuning represents a sophisticated approach to model optimization, allowing for the adaptation of pre-trained neural networks to new tasks or datasets. By combining selective layer freezing, hyperparameter optimization, and iterative model refinement, I successfully enhanced the performance of the InceptionResNetV2 model on the CIFAR-10 dataset, underscoring the efficacy of fine-tuning in deep learning endeavors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "279d2ec0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 Complete [01h 08m 37s]\n",
      "val_accuracy: 0.9447000026702881\n",
      "\n",
      "Best val_accuracy So Far: 0.9447000026702881\n",
      "Total elapsed time: 11h 58m 20s\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 288 and the optimal learning rate for the optimizer\n",
      "is 0.0001.\n",
      "\n",
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 15606s 10s/step - loss: 1.2794 - accuracy: 0.8840 - val_loss: 0.9558 - val_accuracy: 0.9284\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 398s 255ms/step - loss: 0.8213 - accuracy: 0.9377 - val_loss: 0.7363 - val_accuracy: 0.9292\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 398s 254ms/step - loss: 0.5653 - accuracy: 0.9587 - val_loss: 0.5873 - val_accuracy: 0.9335\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 399s 255ms/step - loss: 0.3964 - accuracy: 0.9699 - val_loss: 0.5880 - val_accuracy: 0.9216\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 397s 254ms/step - loss: 0.2782 - accuracy: 0.9786 - val_loss: 0.3930 - val_accuracy: 0.9412\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 400s 256ms/step - loss: 0.1985 - accuracy: 0.9849 - val_loss: 0.3865 - val_accuracy: 0.9394\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 407s 261ms/step - loss: 0.1454 - accuracy: 0.9878 - val_loss: 0.3529 - val_accuracy: 0.9322\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 406s 260ms/step - loss: 0.1084 - accuracy: 0.9909 - val_loss: 0.3301 - val_accuracy: 0.9451\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 407s 261ms/step - loss: 0.0881 - accuracy: 0.9913 - val_loss: 0.2938 - val_accuracy: 0.9338\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 413s 264ms/step - loss: 0.0685 - accuracy: 0.9926 - val_loss: 0.3496 - val_accuracy: 0.9274\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "import tensorflow.keras as K\n",
    "import numpy as np\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(X, Y):\n",
    "    X = tf.keras.applications.inception_resnet_v2.preprocess_input(X)\n",
    "    y = tf.keras.utils.to_categorical(Y, 10)\n",
    "    return X, y\n",
    "\n",
    "# Load CIFAR-10\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# preprocess data CIFAR10\n",
    "X_train, y_train = preprocess_data(X_train, y_train)\n",
    "X_test, y_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "# Function to create Model for Keras Tunning\n",
    "def model_builder(hp):\n",
    "    base_model = keras.applications.InceptionResNetV2(weights='imagenet',\n",
    "                                                       include_top=False,\n",
    "                                                       input_shape=(299, 299, 3))\n",
    "    \n",
    "    \n",
    "    # freeze some layer (before 633)\n",
    "    for layer in base_model.layers[:633]:\n",
    "        layer.trainable=False\n",
    "\n",
    "    for layer in base_model.layers[633:]:\n",
    "        layer.trainable=True\n",
    "    \n",
    "    # Define a function for adding regularizers\n",
    "    def add_regularization(layer, hp):\n",
    "        if hp.Choice('regularization_type', ['l2', 'l1', 'none']) == 'l2':\n",
    "            return keras.layers.Dense(\n",
    "                units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=keras.regularizers.l2(hp.Choice('l2_rate', [0.001, 0.01, 0.1])),\n",
    "            )(layer)\n",
    "        elif hp.Choice('regularization_type', ['l2', 'l1', 'none']) == 'l1':\n",
    "            return keras.layers.Dense(\n",
    "                units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=keras.regularizers.l1(hp.Choice('l1_rate', [0.001, 0.01, 0.1])),\n",
    "            )(layer)\n",
    "        else:\n",
    "            return keras.layers.Dense(\n",
    "                units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                activation='relu',\n",
    "            )(layer)\n",
    "        \n",
    "    # resize image\n",
    "    inputs = K.Input(shape=(32, 32, 3))\n",
    "    input = K.layers.Lambda(lambda image: tf.image.resize(image, (299, 299)))(inputs)\n",
    "    \n",
    "    # construct model\n",
    "    x = base_model(input, training=False)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Add regularized dense layers\n",
    "    for _ in range(hp.Int('num_layers', 1, 5)):\n",
    "        x = add_regularization(x, hp)\n",
    "        x = keras.layers.Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1))(x)\n",
    "    \n",
    "    outputs = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "                    hp.Choice('learning_rate',\n",
    "                              values=[1e-2, 1e-3, 1e-4, 1e-5])),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner and perform hyperparameter search\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print Best parameter\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"Number of units in the first densely-connected layer:\", best_hps.get('units'))\n",
    "print(\"Learning rate:\", best_hps.get('learning_rate'))\n",
    "print(\"Dropout rate:\", best_hps.get('dropout'))  # Add this line to print dropout rate\n",
    "print(\"L2 regularization rate:\", best_hps.get('l2_rate')) \n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# save best model\n",
    "model.save('cifar10_best.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291adc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "Number of units in the first densely-connected layer: 288\n",
      "Learning rate: 0.0001\n",
      "Dropout rate: 0.30000000000000004\n",
      "L2 regularization rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters:\")\n",
    "print(\"Number of units in the first densely-connected layer:\", best_hps.get('units'))\n",
    "print(\"Learning rate:\", best_hps.get('learning_rate'))\n",
    "print(\"Dropout rate:\", best_hps.get('dropout'))  # Add this line to print dropout rate\n",
    "print(\"L2 regularization rate:\", best_hps.get('l2_rate')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfea09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
